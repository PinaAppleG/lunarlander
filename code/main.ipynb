{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In LunarLanderContinious there are two actions possible (values in [-1,1])\n",
    "\n",
    "- First action, main engine:\n",
    "    - [-1..0] => off\n",
    "    - [0..+1] => throttle from 50% to 100% power\n",
    "    \n",
    "- Second action\n",
    "    - [-1.0..-0.5] => fire left engine\n",
    "    - [+0.5..+1.0] => fire right engine\n",
    "    - [-0.5..0.5] => off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Convolution2D\n",
    "from keras.optimizers import sgd\n",
    "\n",
    "import gym, tqdm\n",
    "\n",
    "# Custom files, please check repo\n",
    "from _util import preprocess_env, phi\n",
    "from memory import ReplayMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Initialization of environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nb_frames = 4\n",
    "\n",
    "height = 80\n",
    "width = 120\n",
    "\n",
    "# We need to create bins for the number of actions, we have continuous values, for example here we need\n",
    "# to specify the 10 actions we want to provide. We have to rediscuss this approach because we will be limited to \n",
    "# the actions we define...\n",
    "nb_actions = 4 \n",
    "\n",
    "hidden_size = 32\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(8,8,8,input_shape = (nb_frames, height, width)))\n",
    "model.add(Convolution2D(16,4,4))\n",
    "model.add(Convolution2D(16,3,3))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(hidden_size, activation='relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "\n",
    "model.compile(optimizer='adagrad', loss = \"mse\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN with Experience replay\n",
    "- We got an issue when game finishes, the model predicts a sequence of nan...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "# Initialize replay memory D to capacity N\n",
    "N  = 10\n",
    "replay_memory = ReplayMemory(N)\n",
    "\n",
    "# TODO Initialize action-value function Q with random weights, create bins because of continious function?\n",
    "\n",
    "\n",
    "M = 10 # Number of episodes\n",
    "T = 100 # To be defined\n",
    "epsilon = 0.01 # Exploration\n",
    "gamma = 0.7 \n",
    "\n",
    "loss = []\n",
    "\n",
    "for episode in range(0,M):\n",
    "    env.reset()\n",
    "    \n",
    "    ## Initialise sequence s(1) = {x1}\n",
    "    s = [preprocess_env(env)]\n",
    "\n",
    "    for t in range(0,T):\n",
    "        \n",
    "        \n",
    "        # Choice of action\n",
    "        if np.random.rand()<epsilon:\n",
    "            # Choose action randomly\n",
    "            action_t = np.random.randint(4)\n",
    "        else:\n",
    "            # Choose max(Q(a,s))\\a\n",
    "            q = model.predict(phi(s)[None,:,:,:])[0]\n",
    "            action_t = np.argmax(q)\n",
    "        \n",
    "            \n",
    "        # Execute action at in emulator and observe reward rt and image xt+1\n",
    "        observation, r_t, done, info = env.step(action_t)\n",
    "        x_t_plus_1 = preprocess_env(env) # Renders, downsamples and converts to grayscale the gameview\n",
    "        \n",
    "        # Set st+1 = st; at; xt+1 \n",
    "        tmp = s # st\n",
    "        s.append(action_t)\n",
    "        s.append(x_t_plus_1)\n",
    "        \n",
    "        # Store preprocessed with Phi transition Phi t , at, rt, Phi t+1  in D\n",
    "        replay_memory.append([phi(tmp),action_t,r_t,phi(s)], env.game_over)\n",
    "        \n",
    "        # Sample random minibatch of transitions Phi j , aj , rj , Phi j+1  from D\n",
    "        batch, batch_state = replay_memory.mini_batch(size = 4)\n",
    "        \n",
    "        for j,transition in enumerate(batch):\n",
    "         \n",
    "            phi_j = np.array(batch[j][0])\n",
    "            # Converting to correct size for Keras\n",
    "            phi_j = phi_j[None,:,:,:]\n",
    "            # Output of size 4\n",
    "            q_t = model.predict(phi_j)[0]\n",
    "            \n",
    "\n",
    "            if batch_state[j] == True:\n",
    "                # Terminal j+1\n",
    "                y_j = [transition[2]]*nb_actions\n",
    "            else:\n",
    "                # Non-terminal j+1\n",
    "                phi_j_plus_1 = np.array(batch[j][3])\n",
    "                phi_j_plus_1 = phi_j_plus_1[None,:,:,:]\n",
    "                q_t_plus_1 = model.predict(phi_j_plus_1)[0]\n",
    "                \n",
    "                max_idx = np.argmax(q_t_plus_1)\n",
    "                max_val = q_t_plus_1[max_idx]\n",
    "               \n",
    "                # rj + gamma * max a0 Q(j+1; a0; theta)\n",
    "                y_j = q_t \n",
    "                y_j[max_idx] = transition[2]+gamma*max_val\n",
    "                \n",
    "            #Perform a gradient descent step on (yj - Q(j ; aj ; theta))2 according to equation 3\n",
    "            callback = model.fit(phi_j,np.array(y_j)[None,:],nb_epoch=1)\n",
    "            loss.append(callback.history['loss'])\n",
    "            \n",
    "        if env.game_over:\n",
    "            break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env.reset()\n",
    "s = [preprocess_env(env)]\n",
    "cum_reward = 0\n",
    "for t in range(0,T):\n",
    "    \n",
    "    q = model.predict(phi(s)[None,:,:,:])[0]\n",
    "    action_t = np.argmax(q)\n",
    "\n",
    "    # Execute action at in emulator and observe reward rt and image xt+1\n",
    "    observation, r_t, done, info = env.step(action_t)\n",
    "    env.render()\n",
    "    \n",
    "    cum_reward += gamma**t*r_t\n",
    "    \n",
    "    x_t_plus_1 = preprocess_env(env) # Renders, downsamples and converts to grayscale the gameview\n",
    "   \n",
    "    # Set st+1 = st; at; xt+1 \n",
    "    s.append(action_t)\n",
    "    s.append(x_t_plus_1)\n",
    "    \n",
    "    if env.game_over:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.save_weights('../weights.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
