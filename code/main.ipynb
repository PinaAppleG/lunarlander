{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In LunarLanderContinious there are two actions possible (values in [-1,1])\n",
    "\n",
    "- First action, main engine:\n",
    "    - [-1..0] => off\n",
    "    - [0..+1] => throttle from 50% to 100% power\n",
    "    \n",
    "- Second action\n",
    "    - [-1.0..-0.5] => fire left engine\n",
    "    - [+0.5..+1.0] => fire right engine\n",
    "    - [-0.5..0.5] => off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.optimizers import sgd\n",
    "\n",
    "import gym\n",
    "\n",
    "# Custom files, please check repo\n",
    "from _util import preprocess_env, phi\n",
    "from memory import ReplayMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Initialization of environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env = gym.make('LunarLanderContinuous-v2')\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nb_frames = 4\n",
    "\n",
    "height = 80\n",
    "width = 120\n",
    "\n",
    "# We need to create bins for the number of actions, we have continuous values, for example here we need\n",
    "# to specify the 10 actions we want to provide. We have to rediscuss this approach because we will be limited to \n",
    "# the actions we define...\n",
    "nb_actions = 10 \n",
    "\n",
    "hidden_size = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(nb_frames, height, width)))\n",
    "model.add(Dense(hidden_size, activation='relu'))\n",
    "model.add(Dense(hidden_size, activation='relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.compile(sgd(lr=.2), \"mse\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN with Experience replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize replay memory D to capacity N\n",
    "N  = 100\n",
    "replay_memory = ReplayMemory(N)\n",
    "\n",
    "# TODO Initialize action-value function Q with random weights, create bins because of continious function?\n",
    "\n",
    "\n",
    "M = 10 # Number of episodes\n",
    "T = 10 # To be defined\n",
    "epsilon = 0.01 # Exploration\n",
    "gamma = 0.7 \n",
    "\n",
    "for episode in range(0,M):\n",
    "    ## Initialise sequence s(1) = {x1}\n",
    "        \n",
    "    s = [preprocess_env(env)]\n",
    "\n",
    "    for t in range(0,T):\n",
    "        \n",
    "        # Choice of action\n",
    "        if np.random.rand()<epsilon:\n",
    "            # Choose action randomly\n",
    "            action_t = np.random.rand(2)\n",
    "        else:\n",
    "            # Choose max(Q(a,s))\\a\n",
    "            ## TODO\n",
    "            action_t = np.random.rand(2)\n",
    "        \n",
    "            \n",
    "        # Execute action at in emulator and observe reward rt and image xt+1\n",
    "        observation, r_t, done, info = env.step(action_t)\n",
    "        x_t_plus_1 = preprocess_env(env) # Renders, downsamples and converts to grayscale the gameview\n",
    "        \n",
    "        # Set st+1 = st; at; xt+1 \n",
    "        tmp = s # st\n",
    "        s.append(action_t)\n",
    "        s.append(x_t_plus_1)\n",
    "        \n",
    "        # Store preprocessed with Phi transition Phi t , at, rt, Phi t+1  in D\n",
    "        replay_memory.append([phi(tmp),action_t,r_t,phi(s)], env.game_over)\n",
    "        \n",
    "        # Sample random minibatch of transitions Phi j , aj , rj , Phi j+1  from D\n",
    "        batch, batch_state = replay_memory.mini_batch(size = 4)\n",
    "        \n",
    "        y = []\n",
    "        for j,transition in enumerate(batch):\n",
    "            \n",
    "            if batch_state[j] == True:\n",
    "                # Terminal j+1\n",
    "                y.append(transition[2]) \n",
    "            else:\n",
    "                # Non-terminal j+1\n",
    "                phi_j_plus_1 = np.array(batch[j][3])\n",
    "                phi_j_plus_1 = phi_j_plus_1[None,:,:,:]\n",
    "                \n",
    "                output = model.predict(phi_j_plus_1)\n",
    "                max_val = max(output)\n",
    "                \n",
    "                # rj + gamma * max a0 Q(j+1; a0; theta)\n",
    "                y.append(transition[2]+gamma*max_val)\n",
    "                \n",
    "                #Perform a gradient descent step on (yj - Q(j ; aj ; theta))2 according to equation 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
